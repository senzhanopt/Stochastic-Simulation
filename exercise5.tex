\newpage
\section*{Exercise 5 (comments by Sen)}

On table \ref{tab:week5} the different estimates of $\int_0^1e^{x}dx$ can be seen. We assume that most of the computational cost comes from generating random variables. Hence in order to get comparable computer resources $100$ samples were taken for Crude, Antithetic and control variables. In using stratisfied sampling we have chosen to split the intervals into $10$ sub-intervals and hence we will sample $10$ random variables for each sampling. For this reason stratisfied sampling will be run $10$ times to have comparable computation cost. Also included are relevant comments and in the end of this section it is describes how control variables can be used to reduce the variance for the Poisson process from exercise 4. 

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
 Method & Lower CI & Estimate & Upper CI  \\ \hline
 True value & & 1.7183 & \\ \hline
 Crude Monte Carlo&1.6796&1.7124&1.7452\\ \hline
 Antithetic variables & 1.7134 & 1.7182 & 1.7229 \\ \hline
Control variate & 1.6615 & 1.7176 &1.7737\\ \hline
 Stratified sampling & 1.7138 & 1.7167 & 1.7195 \\ \hline

\end{tabular}
\caption{Estimates of $\int_{0}^{1}e^{x}dx$ for different methods a $95\%$-confidence interval was used. }
\label{tab:week5}
\end{table}

\subsection*{Crude Monte Carlo estimator}
\begin{python}
n = 10
crudeSet = [0] * n
sample = 100
for i in range(n):
    crudeSet[i] = sum( [math.exp(npr.random()) for i\
                        in range(sample) ]) / sample 
\end{python}

\subsection*{Antithetic variables}
\begin{python}
# Antithetic
n = 10
Nsamples = 100
intAnti = np.zeros(n)

for j in range(n):
    Yhat = 0
    for i in range(Nsamples):
        U = npr.rand()
        Yi = (np.exp(U) + np.exp(1-U))/2
        Yhat = Yi + Yhat
    Yhat = Yhat / Nsamples
    intAnti[j] = Yhat
CintAnti = Cint(intAnti,0.05)
\end{python}



\subsection*{Control variate}
\begin{python}
cvSet = [0] * n
for i in range(n):
    cvSet[i] = sum( [(math.exp(npr.random()) - \
                      1.69 * (npr.random() - 0.5))\
                     for i in range(sample) ]) / sample   
\end{python}

\subsection*{Stratisfied sampling}
\begin{python}
# Stratisfied sampling
n = 10
Nsamples = 10
intStrat = np.zeros(n)
for j in range(n):
    What = 0
    for i in range(Nsamples):
        U = np.array([npr.rand() for q in range(10)])/10
        Term = np.array([q/10 for q in range(10)])
        W = np.sum(np.exp(U+Term))/10
        What = W + What
    What = What / Nsamples
    intStrat[j] = What
CintStrat = Cint(intStrat,0.05)
\end{python}

\subsection*{Comments on control-variate}
In the control variate, the parameter c is calculated as in order to minimize the variance.
\begin{equation}
    c = - \frac{\Cov(X, Y)}{\Var(Y)} = - \frac{\E(Ue^U)-\E(U)\E(e^U)}{\E(U^2)-\E(U)^2} = -\frac{1-0.5(e-1)}{\frac{1}{3}-\frac{1}{4}} = -1.69
\end{equation}

\subsection*{Use of control variables to reduce variance of estimator of exercise 4 (Comments by Qahir)}

Let $R$ denote a stochastic variable of the rejection rate we will try with control variables $R^2$, $\exp(R^2)$ and $\log(R^2)$. Where we now define:
\begin{align}
    Z_1 &= R + c_1 (R^2 - \E[R^2])\\
    Z_2 &= R + c_2 (\log(R)-\E[\log(R)]) \\
    Z_3 &= R + c_3 (\exp(R)-\E[\exp(R)])
\end{align}
The constants which minimize the variance are given by the following analytic formulas.
\begin{align}
    c_1 & = -\frac{\Cov(R,R^2)}{\Var(R^2)} \\
    c_2 & = -\frac{\Cov(R,\log(R))}{\Var(\log(R))} \\
    c_3 & = -\frac{\Cov(R,\exp(R))}{\Var(\exp(R))}
\end{align}
It is not feasible to determine the involved statistics analytically. Instead they will be determined from the result of running $1000$ folds of $10000$ samples. This is a lot of folds but this was chosen so that the estimated statistics are as close to their true value as possible so one can ignore the propagation of the uncertainty. The burn-in was set to be $1000$ iterations. Hereafter using the estimates of the covariances, variances and means $10000$ samples on $10$ folds will be run to estimate the confidence interval. Also included is the result when running $1010$ folds for no blocking to obtain a comparison that is fair in terms of computational cost.  The results can be seen on table \ref{tab:week53}. 

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
 Method / Blocking variable & Lower CI & Estimate & Upper CI  \\ \hline
 Erlangs B-formula & & 0.1216610643 & \\ \hline
 None ($10$ folds) &0.11913816 & 0.12228 & 0.12542184\\ \hline
 None ($1010$ folds) & 0.12089432 & 0.12125336 & 0.12161241\\ \hline
 $R^2$ & 0.12161418 & 0.1216999 & 0.12178561\\ \hline
 $\log(R)$ & 0.12148456 & 0.12157371 & 0.12166287 \\  \hline
 $\exp(R)$ & 0.12162756 & 0.12164055 & 0.12165353 \\ \hline
\end{tabular}
\caption{ $95$\% confidence interval for estimation of $R$ different blocking methods}
\label{tab:week53}
\end{table}
It is seen that when we are using blocking true value from Erlangs B-formula is included in all intervals but $\exp(R)$ where there is a deviation. It can be seen that $\exp(R)$ has the smallest variance and this is probably due to this reason since there is still some uncertainty in the estimated statistics. When no blocking is performed it is seen that the true value is captured for $10$ folds but not for $1010$ folds. Running for $1010$ folds again will produce a confidence interval where the true value is contained. We have however chosen to include the results for this case to prove a point - namely that even when making the computational costs for blocking and no-blocking comparable it is still preferable to use blocking. This can be seen from the length of the confidence intervals which are lower for when we are blocking. Hence we are much more likely to have have the true value fall inside the confidence interval when using blocking. Even when taking computational cost into account. 